"""
é«˜çº§æµ‹è¯„ç»“æœåˆ†æç³»ç»Ÿ
æä¾›è¯¦ç»†çš„ç»Ÿè®¡åˆ†æã€æ€§èƒ½æŒ‡æ ‡å’Œå¯è§†åŒ–æ•°æ®
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import json
import os

class AdvancedAnalytics:
    def __init__(self):
        self.score_dimensions = ['å‡†ç¡®æ€§', 'ç›¸å…³æ€§', 'å®‰å…¨æ€§', 'åˆ›é€ æ€§']
    
    def _convert_to_serializable(self, obj: Any) -> Any:
        """è½¬æ¢pandas/numpyæ•°æ®ç±»å‹ä¸ºJSONå¯åºåˆ—åŒ–çš„PythonåŸç”Ÿç±»å‹"""
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._convert_to_serializable(item) for item in obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
        
    def analyze_evaluation_results(self, result_file: str, evaluation_data: Dict = None) -> Dict:
        """
        æ·±åº¦åˆ†æè¯„æµ‹ç»“æœ
        
        Args:
            result_file: ç»“æœæ–‡ä»¶è·¯å¾„
            evaluation_data: è¯„æµ‹å…ƒæ•°æ®
            
        Returns:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        try:
            if not os.path.exists(result_file):
                return {'error': 'ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}
                
            df = pd.read_csv(result_file)
            
            analysis = {
                'basic_stats': self._calculate_basic_stats(df),
                'score_analysis': self._analyze_scores(df),
                'performance_metrics': self._calculate_performance_metrics(df, evaluation_data),
                'quality_indicators': self._calculate_quality_indicators(df),
                'model_comparison': self._compare_models(df),
                'question_type_analysis': self._analyze_by_question_type(df),
                'time_analysis': self._analyze_time_efficiency(evaluation_data),
                'recommendations': self._generate_recommendations(df)
            }
            
            # è½¬æ¢æ‰€æœ‰æ•°æ®ç±»å‹ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            analysis = self._convert_to_serializable(analysis)
            
            return {'success': True, 'analysis': analysis}
            
        except Exception as e:
            return {'error': f'åˆ†æå¤±è´¥: {str(e)}'}
    
    def _calculate_basic_stats(self, df: pd.DataFrame) -> Dict:
        """è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_questions': len(df),
            'completed_questions': 0,
            'response_rate': 0,
            'data_quality_score': 0
        }
        
        # è®¡ç®—å®Œæˆç‡
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        if score_columns:
            completed_count = 0
            for col in score_columns:
                completed_count += df[col].notna().sum()
            stats['completed_questions'] = completed_count
            stats['response_rate'] = (completed_count / (len(df) * len(score_columns))) * 100
        
        # æ•°æ®è´¨é‡è¯„åˆ†
        quality_factors = []
        
        # 1. æ•°æ®å®Œæ•´æ€§
        completeness = df.notna().mean().mean()
        quality_factors.append(completeness)
        
        # 2. è¯„åˆ†ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼‰
        if len(score_columns) > 1:
            score_data = df[score_columns].dropna()
            if len(score_data) > 0:
                consistency = 1 - (score_data.std(axis=1).mean() / 5)  # å½’ä¸€åŒ–åˆ°0-1
                quality_factors.append(max(0, consistency))
        
        stats['data_quality_score'] = np.mean(quality_factors) * 100
        
        return stats
    
    def _analyze_scores(self, df: pd.DataFrame) -> Dict:
        """è¯¦ç»†åˆ†æè¯„åˆ†æƒ…å†µ"""
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        
        analysis = {
            'score_distribution': {},
            'model_performance': {},
            'statistical_summary': {},
            'outliers': []
        }
        
        for col in score_columns:
            model_name = col.replace('_è¯„åˆ†', '').replace('è¯„åˆ†', '')
            scores = df[col].dropna()
            
            if len(scores) == 0:
                continue
                
            # åŸºç¡€ç»Ÿè®¡
            analysis['model_performance'][model_name] = {
                'mean_score': float(scores.mean()),
                'median_score': float(scores.median()),
                'std_dev': float(scores.std()),
                'min_score': float(scores.min()),
                'max_score': float(scores.max()),
                'score_count': int(len(scores)),
                'percentiles': {
                    '25th': float(scores.quantile(0.25)),
                    '75th': float(scores.quantile(0.75)),
                    '90th': float(scores.quantile(0.90))
                }
            }
            
            # è¯„åˆ†åˆ†å¸ƒ
            score_distribution = scores.value_counts().sort_index()
            analysis['score_distribution'][model_name] = {
                'distribution': score_distribution.to_dict(),
                'histogram_data': self._create_histogram_data(scores)
            }
            
            # å¼‚å¸¸å€¼æ£€æµ‹
            outliers = self._detect_outliers(scores, df['é—®é¢˜'].iloc[scores.index] if 'é—®é¢˜' in df.columns else None)
            if outliers:
                analysis['outliers'].extend([
                    {'model': model_name, 'question_index': int(idx), 'score': float(score), 'question': str(question)}
                    for idx, score, question in outliers
                ])
        
        return analysis
    
    def _calculate_performance_metrics(self, df: pd.DataFrame, evaluation_data: Dict = None) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'efficiency_score': 0,
            'consistency_score': 0,
            'reliability_score': 0,
            'estimated_time_per_question': 'æœªçŸ¥',
            'throughput': 0
        }
        
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        
        if evaluation_data:
            # æ—¶é—´æ•ˆç‡è®¡ç®—
            start_time = evaluation_data.get('start_time')
            end_time = evaluation_data.get('end_time')
            
            print(f"ğŸ•’ æ—¶é—´æ•°æ®è°ƒè¯•: start_time={start_time}, end_time={end_time}")
            
            if start_time and end_time:
                try:
                    # å¤„ç†å¤šç§æ—¶é—´æ ¼å¼
                    if isinstance(start_time, str):
                        if 'T' in start_time:
                            start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                        else:
                            start_dt = datetime.fromisoformat(start_time)
                    else:
                        start_dt = start_time
                    
                    if isinstance(end_time, str):
                        if 'T' in end_time:
                            end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                        else:
                            end_dt = datetime.fromisoformat(end_time)
                    else:
                        end_dt = end_time
                    
                    total_time = (end_dt - start_dt).total_seconds()
                    print(f"â±ï¸ è®¡ç®—å¾—åˆ°æ€»æ—¶é—´: {total_time}ç§’")
                    
                    if total_time > 0:
                        time_per_question = total_time / len(df)
                        metrics['estimated_time_per_question'] = f"{time_per_question:.1f}ç§’"
                        metrics['throughput'] = 3600 / time_per_question  # æ¯å°æ—¶å¤„ç†é¢˜æ•°
                        
                        print(f"ğŸ“Š æ—¶é—´æŒ‡æ ‡: æ¯é¢˜{time_per_question:.1f}ç§’, ååé‡{metrics['throughput']:.1f}é¢˜/å°æ—¶")
                        
                        # æ•ˆç‡è¯„åˆ† (åŸºäºå¤„ç†é€Ÿåº¦)
                        if time_per_question < 10:
                            metrics['efficiency_score'] = 95
                        elif time_per_question < 30:
                            metrics['efficiency_score'] = 80
                        elif time_per_question < 60:
                            metrics['efficiency_score'] = 65
                        else:
                            metrics['efficiency_score'] = 40
                        
                        print(f"ğŸ¯ æ•ˆç‡è¯„åˆ†: {metrics['efficiency_score']}")
                    else:
                        print("âš ï¸ è®¡ç®—çš„æ€»æ—¶é—´<=0")
                            
                except Exception as e:
                    print(f"âŒ æ—¶é—´è®¡ç®—é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print("âš ï¸ ç¼ºå°‘æ—¶é—´æ•°æ®: start_timeæˆ–end_timeä¸ºç©º")
        else:
            print("âš ï¸ evaluation_dataä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æ—¶é—´æŒ‡æ ‡")
        
        # ä¸€è‡´æ€§è¯„åˆ†
        if len(score_columns) > 1:
            score_matrix = df[score_columns].dropna()
            if len(score_matrix) > 0:
                # è®¡ç®—æ¨¡å‹é—´è¯„åˆ†ç›¸å…³æ€§
                correlations = score_matrix.corr()
                avg_correlation = correlations.values[np.triu_indices_from(correlations.values, k=1)].mean()
                metrics['consistency_score'] = max(0, avg_correlation * 100)
        
        # å¯é æ€§è¯„åˆ† (åŸºäºæ•°æ®å®Œæ•´æ€§å’Œåˆ†å¸ƒåˆç†æ€§)
        reliability_factors = []
        
        for col in score_columns:
            scores = df[col].dropna()
            if len(scores) > 0:
                # æ£€æŸ¥è¯„åˆ†åˆ†å¸ƒæ˜¯å¦åˆç†
                score_range = scores.max() - scores.min()
                score_std = scores.std()
                
                # åˆç†çš„æ ‡å‡†å·®èŒƒå›´ (0.5-2.0)
                std_factor = min(1.0, max(0.0, (2.0 - abs(score_std - 1.25)) / 2.0))
                reliability_factors.append(std_factor)
        
        if reliability_factors:
            metrics['reliability_score'] = np.mean(reliability_factors) * 100
        
        return metrics
    
    def _calculate_quality_indicators(self, df: pd.DataFrame) -> Dict:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        indicators = {
            'data_completeness': 0,
            'score_validity': 0,
            'distribution_health': {},
            'quality_grade': 'C'
        }
        
        # æ•°æ®å®Œæ•´æ€§
        total_cells = df.shape[0] * df.shape[1]
        filled_cells = df.notna().sum().sum()
        indicators['data_completeness'] = (filled_cells / total_cells) * 100
        
        # è¯„åˆ†æœ‰æ•ˆæ€§
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        valid_scores = 0
        total_scores = 0
        
        for col in score_columns:
            scores = df[col].dropna()
            valid_count = ((scores >= 0) & (scores <= 5)).sum()
            valid_scores += valid_count
            total_scores += len(scores)
        
        if total_scores > 0:
            indicators['score_validity'] = (valid_scores / total_scores) * 100
        
        # åˆ†å¸ƒå¥åº·åº¦
        for col in score_columns:
            model_name = col.replace('_è¯„åˆ†', '').replace('è¯„åˆ†', '')
            scores = df[col].dropna()
            
            if len(scores) > 0:
                # æ£€æŸ¥åˆ†å¸ƒæ˜¯å¦è¿‡äºé›†ä¸­
                distribution = scores.value_counts()
                max_concentration = distribution.max() / len(scores)
                
                health_score = 100
                if max_concentration > 0.8:
                    health_score = 30  # è¿‡äºé›†ä¸­
                elif max_concentration > 0.6:
                    health_score = 60  # è¾ƒé›†ä¸­
                elif max_concentration < 0.1:
                    health_score = 70  # è¿‡äºåˆ†æ•£
                
                indicators['distribution_health'][model_name] = health_score
        
        # ç»¼åˆè´¨é‡ç­‰çº§
        overall_score = (
            indicators['data_completeness'] * 0.3 +
            indicators['score_validity'] * 0.4 +
            np.mean(list(indicators['distribution_health'].values()) or [50]) * 0.3
        )
        
        if overall_score >= 90:
            indicators['quality_grade'] = 'A+'
        elif overall_score >= 85:
            indicators['quality_grade'] = 'A'
        elif overall_score >= 75:
            indicators['quality_grade'] = 'B'
        elif overall_score >= 65:
            indicators['quality_grade'] = 'C'
        else:
            indicators['quality_grade'] = 'D'
        
        return indicators
    
    def _compare_models(self, df: pd.DataFrame) -> Dict:
        """æ¨¡å‹å¯¹æ¯”åˆ†æ"""
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        
        comparison = {
            'ranking': [],
            'performance_gaps': {},
            'strengths_weaknesses': {},
            'recommendation': ''
        }
        
        if len(score_columns) < 2:
            return comparison
        
        # è®¡ç®—æ¨¡å‹æ’å
        model_scores = {}
        for col in score_columns:
            model_name = col.replace('_è¯„åˆ†', '').replace('è¯„åˆ†', '')
            scores = df[col].dropna()
            if len(scores) > 0:
                model_scores[model_name] = {
                    'avg_score': scores.mean(),
                    'consistency': 1 / (scores.std() + 0.1),  # ä¸€è‡´æ€§æŒ‡æ ‡
                    'completion_rate': len(scores) / len(df)
                }
        
        # ç»¼åˆè¯„åˆ†æ’å
        for model, metrics in model_scores.items():
            composite_score = (
                metrics['avg_score'] * 0.6 +
                metrics['consistency'] * 0.2 +
                metrics['completion_rate'] * 5 * 0.2
            )
            comparison['ranking'].append({
                'model': model,
                'composite_score': composite_score,
                'avg_score': metrics['avg_score'],
                'consistency': metrics['consistency'],
                'completion_rate': metrics['completion_rate'] * 100
            })
        
        comparison['ranking'].sort(key=lambda x: x['composite_score'], reverse=True)
        
        # æ€§èƒ½å·®è·åˆ†æ
        if len(comparison['ranking']) >= 2:
            best = comparison['ranking'][0]
            worst = comparison['ranking'][-1]
            comparison['performance_gaps']['score_gap'] = best['avg_score'] - worst['avg_score']
            comparison['performance_gaps']['consistency_gap'] = best['consistency'] - worst['consistency']
        
        return comparison
    
    def _analyze_by_question_type(self, df: pd.DataFrame) -> Dict:
        """æŒ‰é¢˜ç›®ç±»å‹åˆ†æ"""
        if 'ç±»å‹' not in df.columns:
            return {'error': 'ç¼ºå°‘é¢˜ç›®ç±»å‹åˆ—'}
        
        type_analysis = {
            'type_distribution': {},
            'type_performance': {},
            'difficulty_ranking': []
        }
        
        # ç±»å‹åˆ†å¸ƒ
        type_counts = df['ç±»å‹'].value_counts()
        type_analysis['type_distribution'] = type_counts.to_dict()
        
        # å„ç±»å‹çš„æ€§èƒ½è¡¨ç°
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        
        for question_type in df['ç±»å‹'].unique():
            if pd.isna(question_type):
                continue
                
            type_data = df[df['ç±»å‹'] == question_type]
            type_performance = {}
            
            for col in score_columns:
                model_name = col.replace('_è¯„åˆ†', '').replace('è¯„åˆ†', '')
                scores = type_data[col].dropna()
                
                if len(scores) > 0:
                    type_performance[model_name] = {
                        'avg_score': float(scores.mean()),
                        'question_count': len(scores),
                        'difficulty_level': self._calculate_difficulty(scores)
                    }
            
            type_analysis['type_performance'][question_type] = type_performance
            
            # è®¡ç®—æ•´ä½“éš¾åº¦
            all_scores = []
            for col in score_columns:
                all_scores.extend(type_data[col].dropna().tolist())
            
            if all_scores:
                avg_difficulty = np.mean(all_scores)
                type_analysis['difficulty_ranking'].append({
                    'type': question_type,
                    'difficulty_score': avg_difficulty,
                    'question_count': len(type_data)
                })
        
        # æŒ‰éš¾åº¦æ’åº
        type_analysis['difficulty_ranking'].sort(key=lambda x: x['difficulty_score'])
        
        return type_analysis
    
    def _analyze_time_efficiency(self, evaluation_data: Dict = None) -> Dict:
        """åˆ†ææ—¶é—´æ•ˆç‡"""
        time_analysis = {
            'total_duration': 'æœªçŸ¥',
            'average_per_question': 'æœªçŸ¥',
            'efficiency_rating': 'æœªè¯„çº§',
            'bottlenecks': [],
            'optimization_suggestions': [],
            'data_source': 'unknown'
        }
        
        if not evaluation_data:
            time_analysis['data_source'] = 'no_data'
            time_analysis['optimization_suggestions'].append('å»ºè®®è®°å½•è¯„æµ‹å¼€å§‹å’Œç»“æŸæ—¶é—´ä»¥è·å¾—å‡†ç¡®çš„æ•ˆç‡åˆ†æ')
            return time_analysis
        
        start_time = evaluation_data.get('start_time')
        end_time = evaluation_data.get('end_time')
        is_estimated = evaluation_data.get('is_estimated', False)
        
        if start_time and end_time:
            try:
                # å¤„ç†ISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²
                if isinstance(start_time, str):
                    # ç§»é™¤æ—¶åŒºä¿¡æ¯å¹¶è§£æ
                    start_time_clean = start_time.replace('Z', '').replace('+00:00', '')
                    if '.' in start_time_clean:
                        start_dt = datetime.fromisoformat(start_time_clean.split('.')[0])
                    else:
                        start_dt = datetime.fromisoformat(start_time_clean)
                else:
                    start_dt = start_time
                
                if isinstance(end_time, str):
                    end_time_clean = end_time.replace('Z', '').replace('+00:00', '')
                    if '.' in end_time_clean:
                        end_dt = datetime.fromisoformat(end_time_clean.split('.')[0])
                    else:
                        end_dt = datetime.fromisoformat(end_time_clean)
                else:
                    end_dt = end_time
                
                duration = end_dt - start_dt
                total_seconds = duration.total_seconds()
                question_count = evaluation_data.get('question_count', 1)
                
                # è®¾ç½®æ•°æ®æºæ ‡è¯†
                time_analysis['data_source'] = 'estimated' if is_estimated else 'actual'
                
                # æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º
                if total_seconds < 60:
                    time_analysis['total_duration'] = f"{total_seconds:.0f}ç§’"
                elif total_seconds < 3600:
                    minutes = total_seconds // 60
                    seconds = total_seconds % 60
                    time_analysis['total_duration'] = f"{minutes:.0f}åˆ†{seconds:.0f}ç§’"
                else:
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    time_analysis['total_duration'] = f"{hours:.0f}å°æ—¶{minutes:.0f}åˆ†"
                
                avg_time = total_seconds / question_count
                time_analysis['average_per_question'] = f"{avg_time:.1f}ç§’"
                
                # æ•ˆç‡è¯„çº§ï¼ˆæ ¹æ®æ˜¯å¦ä¸ºä¼°ç®—æ•°æ®è°ƒæ•´æ ‡å‡†ï¼‰
                if is_estimated:
                    time_analysis['efficiency_rating'] = 'ä¼°ç®—æ•°æ®'
                    time_analysis['optimization_suggestions'].append('æ•°æ®ä¸ºåŸºäºæ–‡ä»¶æ—¶é—´çš„ä¼°ç®—ï¼Œå»ºè®®è®°å½•å®é™…è¯„æµ‹æ—¶é—´ä»¥è·å¾—å‡†ç¡®åˆ†æ')
                else:
                    if avg_time < 10:
                        time_analysis['efficiency_rating'] = 'ä¼˜ç§€'
                    elif avg_time < 30:
                        time_analysis['efficiency_rating'] = 'è‰¯å¥½'
                    elif avg_time < 60:
                        time_analysis['efficiency_rating'] = 'ä¸€èˆ¬'
                    else:
                        time_analysis['efficiency_rating'] = 'éœ€è¦ä¼˜åŒ–'
                        time_analysis['bottlenecks'].append('å¤„ç†æ—¶é—´è¿‡é•¿')
                        time_analysis['optimization_suggestions'].append('è€ƒè™‘å¹¶è¡Œå¤„ç†æˆ–ä¼˜åŒ–APIè°ƒç”¨')
                
                # æ ¹æ®æ€»æ—¶é•¿ç»™å‡ºå»ºè®®
                if total_seconds > 3600:  # è¶…è¿‡1å°æ—¶
                    time_analysis['bottlenecks'].append('æ€»è¯„æµ‹æ—¶é—´è¿‡é•¿')
                    time_analysis['optimization_suggestions'].append('è€ƒè™‘æ‰¹é‡å¤„ç†æˆ–åˆ†æ‰¹è¯„æµ‹')
                
            except Exception as e:
                print(f"æ—¶é—´åˆ†æé”™è¯¯: {e}")
                time_analysis['data_source'] = 'error'
                time_analysis['optimization_suggestions'].append(f'æ—¶é—´æ•°æ®è§£æé”™è¯¯: {str(e)}')
        else:
            time_analysis['data_source'] = 'incomplete'
            time_analysis['optimization_suggestions'].append('ç¼ºå°‘å®Œæ•´çš„å¼€å§‹æˆ–ç»“æŸæ—¶é—´ä¿¡æ¯')
        
        return time_analysis
    
    def _generate_recommendations(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        score_columns = [col for col in df.columns if 'è¯„åˆ†' in col]
        
        # æ•°æ®è´¨é‡å»ºè®®
        completion_rate = df[score_columns].notna().mean().mean()
        if completion_rate < 0.9:
            recommendations.append("å»ºè®®æé«˜æ•°æ®å®Œæ•´æ€§ï¼Œç¡®ä¿æ‰€æœ‰é¢˜ç›®éƒ½å¾—åˆ°è¯„åˆ†")
        
        # è¯„åˆ†åˆ†å¸ƒå»ºè®®
        for col in score_columns:
            scores = df[col].dropna()
            if len(scores) > 0:
                model_name = col.replace('_è¯„åˆ†', '').replace('è¯„åˆ†', '')
                
                # æ£€æŸ¥åˆ†å¸ƒé›†ä¸­åº¦
                distribution = scores.value_counts()
                max_concentration = distribution.max() / len(scores)
                
                if max_concentration > 0.7:
                    recommendations.append(f"{model_name}çš„è¯„åˆ†è¿‡äºé›†ä¸­ï¼Œå»ºè®®æ£€æŸ¥è¯„æµ‹æ ‡å‡†")
                
                # æ£€æŸ¥è¯„åˆ†èŒƒå›´
                score_range = scores.max() - scores.min()
                if score_range < 2:
                    recommendations.append(f"{model_name}çš„è¯„åˆ†èŒƒå›´è¾ƒçª„ï¼Œå»ºè®®ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¯„åˆ†æ ‡å‡†")
        
        # æ¨¡å‹å¯¹æ¯”å»ºè®®
        if len(score_columns) > 1:
            model_means = {col: df[col].dropna().mean() for col in score_columns}
            best_model = max(model_means, key=model_means.get)
            worst_model = min(model_means, key=model_means.get)
            
            gap = model_means[best_model] - model_means[worst_model]
            if gap > 1.0:
                recommendations.append(f"æ¨¡å‹æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–{worst_model.replace('_è¯„åˆ†', '')}")
        
        return recommendations
    
    def _create_histogram_data(self, scores: pd.Series) -> Dict:
        """åˆ›å»ºç›´æ–¹å›¾æ•°æ®"""
        bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]
        hist, _ = np.histogram(scores, bins=bins)
        
        return {
            'bins': ['1åˆ†', '2åˆ†', '3åˆ†', '4åˆ†', '5åˆ†'],
            'counts': hist.tolist()
        }
    
    def _detect_outliers(self, scores: pd.Series, questions: pd.Series = None) -> List[Tuple]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        outliers = []
        
        Q1 = scores.quantile(0.25)
        Q3 = scores.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outlier_mask = (scores < lower_bound) | (scores > upper_bound)
        outlier_indices = scores[outlier_mask].index
        
        for idx in outlier_indices:
            question_text = questions.iloc[idx] if questions is not None else f"é¢˜ç›®{idx+1}"
            outliers.append((idx, scores.iloc[idx], question_text))
        
        return outliers
    
    def _calculate_difficulty(self, scores: pd.Series) -> str:
        """è®¡ç®—é¢˜ç›®éš¾åº¦ç­‰çº§"""
        avg_score = scores.mean()
        
        if avg_score >= 4.5:
            return 'ç®€å•'
        elif avg_score >= 3.5:
            return 'ä¸­ç­‰'
        elif avg_score >= 2.5:
            return 'å›°éš¾'
        else:
            return 'æéš¾'

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
analytics = AdvancedAnalytics()
